For implementing a cache I made use of open addressing using linear probing. I chose this over doublehashing as this technique was far more efficient when the table size is as small as 1000 compared to double hashing. I chose linear probing over chaining because in chaining the keys are much more far apart while in linear probing the values with the same keys are placed in a contiguent manner so when cache accesses a chunk of data from the disk, it is more likely that the chunk of data accessed has most of the values with same keys compared to if we were using chaining. This not only makes lookups from cache faster but also mean we need to read from the disk less compared to chaining.Hence linear probing is more cache friendly than chaining. The hashfunction I used in linear probing was the one with the bitwise hash code and division compression as it was the most efficient compared to the other three hashfunction combinations.

The file cache.cpp does not have a linear probing based cache implemented in it but cache1.cpp does. When timing the two implementations, the latter was much faster than the cache-less version because instead iterating throught the whole dictionary file to find the word, we could simply look into a much shorter list containing frequently accessed words. So cache made lookups significantly faster.

(I'm using a virtual machine hence my program has a much higher delay compared to when being run on a linux machine.)